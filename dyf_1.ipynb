{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkoqiqSMe2vRLjZcTUPpaU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1324fgg/GraphAlign/blob/main/dyf_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First section: connect to drive and prepare the environment\n"
      ],
      "metadata": {
        "id": "3ABNia9MxUjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect file to /content/drive, actually you colab folder path is /content/drive/MyDrive/colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Change the current path to our work directory\n",
        "import os\n",
        "path = \"/content/drive/MyDrive/ColabNotebooks/socialScienceProject\"\n",
        "os.chdir(path)\n",
        "os.listdir\n",
        "\n",
        "!nvidia-smi\n",
        "!pip install ogb\n",
        "!pip install torch\n",
        "!pip install dgl==1.1.0 -f https://data.dgl.ai/wheels/repo.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YKqtMNttpNy",
        "outputId": "f4f43b11-aa6c-413d-939b-c891836186a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.11/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.3.0)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Requirement already satisfied: dgl==1.1.0 in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.0) (1.14.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.0) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.0) (4.67.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.1.0) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.1.0) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9DP6t17ACth"
      },
      "outputs": [],
      "source": [
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "\n",
        "mag_dataset = DglNodePropPredDataset(name = \"ogbn-mag\")\n",
        "mag_split_idx = mag_dataset.get_idx_split()\n",
        "mag_train_idx, mag_valid_idx, mag_test_idx = mag_split_idx[\"train\"], mag_split_idx[\"valid\"], mag_split_idx[\"test\"]\n",
        "mag_graph, mag_label = mag_dataset[0] # graph: dgl graph object, label: torch tensor of shape (num_nodes, num_tasks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mag_graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmYMBMnOvf0D",
        "outputId": "cfebd302-2d07-45b7-c9c3-b97445d10454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph(num_nodes={'author': 1134649, 'field_of_study': 59965, 'institution': 8740, 'paper': 736389},\n",
            "      num_edges={('author', 'affiliated_with', 'institution'): 1043998, ('author', 'writes', 'paper'): 7145660, ('paper', 'cites', 'paper'): 5416271, ('paper', 'has_topic', 'field_of_study'): 7505078},\n",
            "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mag_graph)\n",
        "print(mag_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6Rehm30ba3g",
        "outputId": "f85805ef-55c7-436e-9892-3779a4f01f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph(num_nodes={'author': 1134649, 'field_of_study': 59965, 'institution': 8740, 'paper': 736389},\n",
            "      num_edges={('author', 'affiliated_with', 'institution'): 1043998, ('author', 'writes', 'paper'): 7145660, ('paper', 'cites', 'paper'): 5416271, ('paper', 'has_topic', 'field_of_study'): 7505078},\n",
            "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])\n",
            "{'paper': tensor([[246],\n",
            "        [131],\n",
            "        [189],\n",
            "        ...,\n",
            "        [266],\n",
            "        [289],\n",
            "        [  1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "\n",
        "arxiv_dataset = DglNodePropPredDataset(name = \"ogbn-arxiv\")\n",
        "arxiv_split_idx = arxiv_dataset.get_idx_split()\n",
        "arxiv_train_idx, arxiv_valid_idx, arxiv_test_idx = arxiv_split_idx[\"train\"], arxiv_split_idx[\"valid\"], arxiv_split_idx[\"test\"]\n",
        "arxiv_graph, arxiv_label = arxiv_dataset[0] # graph: dgl graph object, label: torch tensor of shape (num_nodes, num_tasks)"
      ],
      "metadata": {
        "id": "8OjZ4q0PyJIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(arxiv_graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFsQCtsXvXl-",
        "outputId": "ba516b2d-d1c3-445e-ef74-9304d009364e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph(num_nodes=169343, num_edges=1166243,\n",
            "      ndata_schemes={'year': Scheme(shape=(1,), dtype=torch.int64), 'feat': Scheme(shape=(128,), dtype=torch.float32)}\n",
            "      edata_schemes={})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Section: we need to output the basic information of this 2 dataset, and sample a samller dataset from the mag with 2w nodes."
      ],
      "metadata": {
        "id": "nrfon84nxc3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Here we generate basic information of 2 dataset"
      ],
      "metadata": {
        "id": "7RZRY9V5QM0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ------------------------- Utility Functions -------------------------\n",
        "def plot_degree_distribution(degrees, title, filename, loglog=False):\n",
        "\n",
        "    degree_counts = Counter(degrees)\n",
        "    x, y = zip(*degree_counts.items())\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(x, y, s=20, alpha=0.6)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xlabel('Degree' if not loglog else 'Degree (log)')\n",
        "    plt.ylabel('Frequency' if not loglog else 'Frequency (log)')\n",
        "\n",
        "    if loglog:\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "    else:\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.savefig(f\"{filename}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def analyze_network(G, network_name):\n",
        "    # ----------------- Basic Properties -----------------\n",
        "    metrics = {\n",
        "        'num_nodes': G.number_of_nodes(),\n",
        "        'num_edges': G.number_of_edges(),\n",
        "        'avg_in_degree': np.mean([d for _, d in G.in_degree()]),\n",
        "        'avg_out_degree': np.mean([d for _, d in G.out_degree()]),\n",
        "        'density': nx.density(G)\n",
        "    }\n",
        "\n",
        "    # ----------------- Degree Distributions -----------------\n",
        "    print(G.number_of_nodes())\n",
        "    print(G.number_of_edges())\n",
        "\n",
        "    all_degrees = list(dict(G.degree()).values())\n",
        "    print(all_degrees)\n",
        "\n",
        "    plot_degree_distribution(all_degrees,\n",
        "                           f\"{network_name} Degree Distribution (Linear)\",\n",
        "                           f\"{network_name}_degree_linear\")\n",
        "    plot_degree_distribution(all_degrees,\n",
        "                           f\"{network_name} Degree Distribution (Log-Log)\",\n",
        "                           f\"{network_name}_degree_log\",\n",
        "                           loglog=True)\n",
        "\n",
        "\n",
        "    # ----------------- Giant Component Analysis -----------------\n",
        "    if nx.is_directed(G):\n",
        "        scc = max(nx.strongly_connected_components(G), key=len)\n",
        "        #gain the giant component, with all the attributes of the original graph\n",
        "        G_gc = G.subgraph(scc)\n",
        "    else:\n",
        "        gcc = max(nx.connected_components(G), key=len)\n",
        "        G_gc = G.subgraph(gcc)\n",
        "\n",
        "    metrics['gc_nodes'] = G_gc.number_of_nodes()\n",
        "    print(G_gc.number_of_nodes())\n",
        "\n",
        "    # ----------------- Path Length Distribution -----------------\n",
        "    path_lengths = []\n",
        "\n",
        "    sample_size = 50\n",
        "    sampled_nodes = random.sample(list(G_gc.nodes()), sample_size)\n",
        "\n",
        "    for node in sampled_nodes:\n",
        "        lengths = nx.single_source_shortest_path_length(G_gc, node)\n",
        "        print\n",
        "        path_lengths.extend(lengths.values())\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.hist(path_lengths, bins=30)\n",
        "    plt.title(f\"{network_name} Path Length Distribution\")\n",
        "    plt.savefig(f\"{network_name}_path_lengths.png\", dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    metrics['avg_path_length'] = np.mean(path_lengths)\n",
        "\n",
        "    # ----------------- Clustering Analysis -----------------\n",
        "    sampled_clustering = {}\n",
        "\n",
        "    # Convert MultiDiGraph to DiGraph\n",
        "    G_simple = nx.DiGraph(G_gc)\n",
        "    for node in sampled_nodes:\n",
        "        sampled_clustering[node] = nx.clustering(G_simple, node)\n",
        "    clustering = sampled_clustering\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.hist(clustering.values(), bins=30)\n",
        "    plt.title(f\"{network_name} Clustering Coefficients\")\n",
        "    plt.savefig(f\"{network_name}_clustering.png\", dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    metrics['avg_clustering'] = np.mean(list(clustering.values()))\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "uP9tETq19jQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "arxiv_G = dgl.to_networkx(arxiv_graph, edge_attrs=None, node_attrs=None)\n",
        "arxiv_metrics = analyze_network(arxiv_G, \"arxiv\")\n",
        "print(arxiv_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "8hKFIWsuCmp9",
        "outputId": "abd093a9-30b4-49d6-feb4-5b8a694e1b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a8d8fa21e6b3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marxiv_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marxiv_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0marxiv_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marxiv_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arxiv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marxiv_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dgl/convert.py\u001b[0m in \u001b[0;36mto_networkx\u001b[0;34m(g, node_attrs, edge_attrs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[0mnx_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m         \u001b[0mnx_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnode_attrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/networkx/classes/multidigraph.py\u001b[0m in \u001b[0;36madd_edge\u001b[0;34m(self, u_for_edge, v_for_edge, key, **attr)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# selfloops work this way without special treatment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mdatadict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m             \u001b[0mdatadict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mkeydict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_key_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "paper_edges = ('paper', 'cites', 'paper')\n",
        "paper_graph = mag_graph.edge_type_subgraph([paper_edges])\n",
        "mag_G = dgl.to_homogeneous(paper_graph)\n",
        "mag_G = dgl.to_networkx(mag_G, edge_attrs=None, node_attrs=None)\n",
        "mag_metrics = analyze_network(mag_G, \"mag\")\n",
        "print(mag_metrics)"
      ],
      "metadata": {
        "id": "fEMTrdmHov2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Here we sample a subgraph of mag containing 2w nodes."
      ],
      "metadata": {
        "id": "rPUbwQYU09vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "\n",
        "def bfs_sampling_with_high_degree(dgl_graph, target_node_limit=200000):\n",
        "    # 选择度数最高的节点作为种子节点\n",
        "    all_degrees = dgl_graph.in_degrees()  # 获取节点的入度\n",
        "    start_node = torch.argmax(all_degrees).item()  # 找到入度最高的节点\n",
        "\n",
        "    # 广度优先搜索（BFS）扩展邻居节点\n",
        "    visited_nodes = set()\n",
        "    queue = [start_node]\n",
        "\n",
        "    while queue and len(visited_nodes) < target_node_limit:\n",
        "        node = queue.pop(0)\n",
        "        if node not in visited_nodes:\n",
        "            visited_nodes.add(node)\n",
        "            queue.extend(dgl_graph.successors(node).tolist())  # 添加邻居节点\n",
        "\n",
        "    # 将访问的节点创建为子图\n",
        "    sampled_nodes = list(visited_nodes)\n",
        "    subgraph = dgl.node_subgraph(dgl_graph, sampled_nodes)\n",
        "    return subgraph\n",
        "\n",
        "#仅保留节点和边，采用dgl对象的edge_type_subgraph的方法\n",
        "paper_edges = ('paper', 'cites', 'paper')\n",
        "paper_graph = mag_graph.edge_type_subgraph([paper_edges])\n",
        "\n",
        "#从异构图转化为同质图，即节点和边都转化成统一类型，需要手动指定year和feat\n",
        "mag_G = dgl.to_homogeneous(paper_graph, ndata=['year', 'feat'], edata=['reltype'])\n",
        "# 示例：使用 BFS 基于高入度节点采样 2 万节点子图\n",
        "sampled_graph = bfs_sampling_with_high_degree(mag_G, target_node_limit=200000)\n",
        "\n",
        "print(mag_graph.ndata)\n",
        "print(paper_graph.ndata)\n",
        "print(sampled_graph.ndata)\n",
        "\n",
        "#add mag_lables to sampled_graph\n",
        "sampled_ids = sampled_graph.ndata['_ID']  # Original IDs of nodes in sampled_graph\n",
        "#paper_labels = mag_label['paper'] #注意这里一定要添加paper才能通过_ID 访问标签\n",
        "# Retrieve labels using original IDs\n",
        "sampled_graph.ndata['y'] = mag_label['paper'][sampled_ids]  # Add labels to sampled graph\n",
        "\n",
        "print(sampled_graph.ndata['y'].shape)\n",
        "\n",
        "#save sampled graph\n",
        "import pickle\n",
        "\n",
        "def save_graph(graph, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(graph, f)\n",
        "    print(f\"Graph saved to {filename}\")\n",
        "\n",
        "# Save your sampled graph\n",
        "save_graph(sampled_graph, \"sampled_graph_mag_200000.pkl\")"
      ],
      "metadata": {
        "id": "9Hlixqe40xwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "#load the saved sampled graph mag:\n",
        "def load_graph(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        graph = pickle.load(f)\n",
        "    print(f\"Graph loaded from {filename}\")\n",
        "    return graph\n",
        "\n",
        "# Load the graph\n",
        "sampled_graph = load_graph(\"sampled_graph_mag_200000.pkl\")\n",
        "\n",
        "# Verify the loaded graph structure\n",
        "print(\"Node data:\", sampled_graph.ndata.keys())\n",
        "print(\"Edge data:\", sampled_graph.edata.keys())\n"
      ],
      "metadata": {
        "id": "TDlTbp6X5Vhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Then we describe the sampled subgraph of mag."
      ],
      "metadata": {
        "id": "zHqRKuo61WpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#转换成network之后可以计算graph的特征并绘图\n",
        "sampled_graph_nx = dgl.to_networkx(sampled_graph, edge_attrs=None, node_attrs=None)\n",
        "mag_metrics = analyze_network(sampled_graph_nx, \"mag_sampledgraph\")\n",
        "\n",
        "# 检查子图信息\n",
        "print(\"Number of nodes in sampled graph:\", sampled_graph.num_nodes())\n",
        "print(\"Number of edges in sampled graph:\", sampled_graph.num_edges())\n"
      ],
      "metadata": {
        "id": "s-_SZFT3qTig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mag_graph)\n",
        "print(paper_graph)\n",
        "print(sampled_graph)"
      ],
      "metadata": {
        "id": "JKPsYYaiwcxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we comes to train the sampled mag graph."
      ],
      "metadata": {
        "id": "_m9XXnd-E4Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Third Section:we can train the graphsage on sampled mag graph using the original embedding."
      ],
      "metadata": {
        "id": "lrAymysNDPOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 First we define the architecture of GraphSAGE"
      ],
      "metadata": {
        "id": "1n3n78egQnNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "from dgl.nn import SAGEConv\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define GraphSAGE model\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats, out_feats, num_layers, dropout):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        self.layers.append(SAGEConv(in_feats, hidden_feats, aggregator_type='mean'))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.layers.append(SAGEConv(hidden_feats, hidden_feats, aggregator_type='mean'))\n",
        "        self.layers.append(SAGEConv(hidden_feats, out_feats, aggregator_type='mean'))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(g, x)\n",
        "            if i != len(self.layers) - 1:  # No activation on the last layer\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "# Training loop\n",
        "def train(model, graph, features, labels, train_idx, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(graph, features)\n",
        "    loss = F.cross_entropy(out[train_idx], labels[train_idx], label_smoothing=0.1)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluation loop\n",
        "@torch.no_grad()\n",
        "def evaluate(model, graph, features, labels, split_idx):\n",
        "    model.eval()\n",
        "    out = model(graph, features)\n",
        "    preds = out.argmax(dim=1)\n",
        "    train_acc = accuracy_score(labels[split_idx['train']].cpu(), preds[split_idx['train']].cpu())\n",
        "    valid_acc = accuracy_score(labels[split_idx['valid']].cpu(), preds[split_idx['valid']].cpu())\n",
        "    test_acc = accuracy_score(labels[split_idx['test']].cpu(), preds[split_idx['test']].cpu())\n",
        "    return train_acc, valid_acc, test_acc"
      ],
      "metadata": {
        "id": "3VukMrT2E40s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sampled_ids)\n",
        "print(mag_train_idx['paper'])"
      ],
      "metadata": {
        "id": "wCdKUir0F4Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Then we split the train dataset, validation dataset and test dataset"
      ],
      "metadata": {
        "id": "IZ_qKhGcQyPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 获取所有节点的年份信息\n",
        "years = sampled_graph.ndata['year'].flatten()  # 将年份信息拉平为一维张量\n",
        "\n",
        "# 使用 PyTorch 或 NumPy 统计年份分布\n",
        "unique_years, year_counts = torch.unique(years, return_counts=True)\n",
        "\n",
        "# 打印年份统计结果\n",
        "print(\"Year Distribution:\")\n",
        "for year, count in zip(unique_years.tolist(), year_counts.tolist()):\n",
        "    print(f\"Year: {year}, Count: {count}\")\n",
        "\n",
        "\n",
        "# 按年份划分训练集、验证集和测试集\n",
        "def split_by_year(graph, years, train_cutoff, valid_cutoff):\n",
        "    # 创建布尔掩码\n",
        "    train_mask = (years <= train_cutoff)  # 截止到 train_cutoff 年份属于训练集\n",
        "    valid_mask = (years > train_cutoff) & (years <= valid_cutoff)  # 在 valid_cutoff 年份之间属于验证集\n",
        "    test_mask = (years > valid_cutoff)  # 剩余的属于测试集\n",
        "\n",
        "    # 将掩码添加到图中\n",
        "    graph.ndata['train_mask'] = train_mask\n",
        "    graph.ndata['valid_mask'] = valid_mask\n",
        "    graph.ndata['test_mask'] = test_mask\n",
        "\n",
        "    return graph\n",
        "\n",
        "# 假设训练集为 2015 年及以前，验证集为 2016-2017 年，测试集为 2018 年及以后\n",
        "train_cutoff_year = 2013\n",
        "valid_cutoff_year = 2015\n",
        "\n",
        "# 更新 sampled_graph 的掩码\n",
        "sampled_graph = split_by_year(sampled_graph, years, train_cutoff_year, valid_cutoff_year)\n",
        "\n",
        "# 验证划分结果\n",
        "print(\"Number of training nodes:\", sampled_graph.ndata['train_mask'].sum().item())\n",
        "print(\"Number of validation nodes:\", sampled_graph.ndata['valid_mask'].sum().item())\n",
        "print(\"Number of testing nodes:\", sampled_graph.ndata['test_mask'].sum().item())\n"
      ],
      "metadata": {
        "id": "v0eVeQTKKcaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Finally we start training"
      ],
      "metadata": {
        "id": "LZ0FKOpaSAIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_sampled_graph(sampled_graph):\n",
        "    # Set up training parameters\n",
        "    features = sampled_graph.ndata['feat']\n",
        "\n",
        "    #notice the shape of the labels. This better be changed during the create of the dataset, I forgot. So need to do this tansform every time.\n",
        "    sampled_labels = sampled_graph.ndata['y']\n",
        "    sampled_labels = sampled_labels.squeeze(1)  # Convert shape from [20000, 1] to [20000]\n",
        "    #sampled_graph.ndata['label'] = sampled_graph.ndata['y'].squeeze(1) #we don't change the original dataset\n",
        "    # Extract masks\n",
        "    train_idx = torch.nonzero(sampled_graph.ndata['train_mask'], as_tuple=True)[0]\n",
        "    valid_idx = torch.nonzero(sampled_graph.ndata['valid_mask'], as_tuple=True)[0]\n",
        "    test_idx = torch.nonzero(sampled_graph.ndata['test_mask'], as_tuple=True)[0]\n",
        "    split_idx = {'train': train_idx, 'valid': valid_idx, 'test': test_idx}\n",
        "\n",
        "    in_feats = features.shape[1]\n",
        "    hidden_feats = 512\n",
        "    out_feats = sampled_labels.max().item() + 1\n",
        "    num_layers = 3\n",
        "    dropout = 0.3\n",
        "\n",
        "    model = GraphSAGE(in_feats, hidden_feats, out_feats, num_layers, dropout)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    print(\"Features shape:\", features.shape)\n",
        "    print(\"Sampled Labels shape:\", sampled_labels.shape)\n",
        "    print(\"Train indices shape:\", train_idx.shape)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(200):  # Example: 100 epochs\n",
        "        loss = train(model, sampled_graph, features, sampled_labels, train_idx, optimizer)\n",
        "        train_acc, valid_acc, test_acc = evaluate(model, sampled_graph, features, sampled_labels, split_idx)\n",
        "        print(f'Epoch {epoch + 1}: Loss = {loss:.4f}, Train Acc = {train_acc:.4f}, '\n",
        "                  f'Valid Acc = {valid_acc:.4f}, Test Acc = {test_acc:.4f}')\n",
        "\n",
        "main_sampled_graph(sampled_graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ti4eBhJFqGw",
        "outputId": "5b84aa33-6173-4039-a22e-608003c9c88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: torch.Size([200000, 128])\n",
            "Sampled Labels shape: torch.Size([200000])\n",
            "Train indices shape: torch.Size([128833])\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  assert input.numel() == input.storage().size(), (\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 6.0597, Train Acc = 0.1116, Valid Acc = 0.1648, Test Acc = 0.1675\n",
            "Epoch 2: Loss = 5.7003, Train Acc = 0.0704, Valid Acc = 0.1335, Test Acc = 0.1860\n",
            "Epoch 3: Loss = 4.9591, Train Acc = 0.1325, Valid Acc = 0.1517, Test Acc = 0.1962\n",
            "Epoch 4: Loss = 5.0384, Train Acc = 0.1325, Valid Acc = 0.1164, Test Acc = 0.1468\n",
            "Epoch 5: Loss = 4.9779, Train Acc = 0.1291, Valid Acc = 0.1071, Test Acc = 0.1311\n",
            "Epoch 6: Loss = 4.8602, Train Acc = 0.1653, Valid Acc = 0.1860, Test Acc = 0.2299\n",
            "Epoch 7: Loss = 4.7279, Train Acc = 0.1696, Valid Acc = 0.2422, Test Acc = 0.3161\n",
            "Epoch 8: Loss = 4.5972, Train Acc = 0.1642, Valid Acc = 0.2378, Test Acc = 0.3102\n",
            "Epoch 9: Loss = 4.4891, Train Acc = 0.1672, Valid Acc = 0.2379, Test Acc = 0.3082\n",
            "Epoch 10: Loss = 4.4044, Train Acc = 0.1818, Valid Acc = 0.2468, Test Acc = 0.3196\n",
            "Epoch 11: Loss = 4.3036, Train Acc = 0.2003, Valid Acc = 0.2477, Test Acc = 0.3153\n",
            "Epoch 12: Loss = 4.2254, Train Acc = 0.2012, Valid Acc = 0.2144, Test Acc = 0.2642\n",
            "Epoch 13: Loss = 4.1309, Train Acc = 0.2074, Valid Acc = 0.2303, Test Acc = 0.2771\n",
            "Epoch 14: Loss = 4.0313, Train Acc = 0.2186, Valid Acc = 0.2689, Test Acc = 0.3193\n",
            "Epoch 15: Loss = 3.9307, Train Acc = 0.2284, Valid Acc = 0.2728, Test Acc = 0.2962\n",
            "Epoch 16: Loss = 3.8736, Train Acc = 0.2322, Valid Acc = 0.2713, Test Acc = 0.2923\n",
            "Epoch 17: Loss = 3.8315, Train Acc = 0.2466, Valid Acc = 0.2756, Test Acc = 0.2928\n",
            "Epoch 18: Loss = 3.7812, Train Acc = 0.2574, Valid Acc = 0.2813, Test Acc = 0.3038\n",
            "Epoch 19: Loss = 3.7340, Train Acc = 0.2600, Valid Acc = 0.2849, Test Acc = 0.3131\n",
            "Epoch 20: Loss = 3.6968, Train Acc = 0.2645, Valid Acc = 0.2894, Test Acc = 0.3105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sampled_graph.ndata['y'].shape)"
      ],
      "metadata": {
        "id": "Q2su4sHsH_IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fourth Section: similarly we train the graphsage on arxiv dataset using original embedding"
      ],
      "metadata": {
        "id": "1hlbAo77VEji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This one is for cpu\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "from dgl.nn import SAGEConv\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#This below is the same with the function in section 3\n",
        "# 定义 GraphSAGE 模型\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats, out_feats, num_layers, dropout):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        self.layers.append(SAGEConv(in_feats, hidden_feats, aggregator_type='mean'))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.layers.append(SAGEConv(hidden_feats, hidden_feats, aggregator_type='mean'))\n",
        "        self.layers.append(SAGEConv(hidden_feats, out_feats, aggregator_type='mean'))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(g, x)\n",
        "            if i != len(self.layers) - 1:  # 最后一层不使用激活\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "# 训练函数\n",
        "def train(model, graph, features, labels, train_idx, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(graph, features)\n",
        "    loss = F.cross_entropy(out[train_idx], labels[train_idx], label_smoothing=0.1)  # 添加标签平滑\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# 验证/测试函数\n",
        "@torch.no_grad()\n",
        "def evaluate(model, graph, features, labels, split_idx):\n",
        "    model.eval()\n",
        "    out = model(graph, features)\n",
        "    preds = out.argmax(dim=1)\n",
        "\n",
        "    train_acc = accuracy_score(labels[split_idx['train']].cpu(), preds[split_idx['train']].cpu())\n",
        "    valid_acc = accuracy_score(labels[split_idx['valid']].cpu(), preds[split_idx['valid']].cpu())\n",
        "    test_acc = accuracy_score(labels[split_idx['test']].cpu(), preds[split_idx['test']].cpu())\n",
        "\n",
        "    return train_acc, valid_acc, test_acc\n",
        "\n",
        "#Only the main function changed\n",
        "# 主函数\n",
        "def main():\n",
        "    # 1. 加载 ogbn-arxiv 数据集\n",
        "    arxiv_dataset = DglNodePropPredDataset(name=\"ogbn-arxiv\")\n",
        "    arxiv_split_idx = arxiv_dataset.get_idx_split()\n",
        "    arxiv_train_idx, arxiv_valid_idx, arxiv_test_idx = (\n",
        "        arxiv_split_idx[\"train\"], arxiv_split_idx[\"valid\"], arxiv_split_idx[\"test\"]\n",
        "    )\n",
        "    arxiv_graph, arxiv_label = arxiv_dataset[0]  # 图和标签\n",
        "\n",
        "    # 2. 数据预处理\n",
        "    arxiv_graph.ndata['label'] = arxiv_label.squeeze(1)  # 将标签存储到图节点数据中\n",
        "    arxiv_graph = dgl.to_bidirected(arxiv_graph, copy_ndata=True)  # 转换为无向图\n",
        "    arxiv_graph = dgl.add_self_loop(arxiv_graph)  # 添加自环\n",
        "    arxiv_graph.create_formats_()  # 转换为稀疏格式，加速计算\n",
        "\n",
        "    # 3. 设置特征和标签\n",
        "    features = arxiv_graph.ndata['feat']\n",
        "    labels = arxiv_graph.ndata['label']\n",
        "    split_idx = {\n",
        "        'train': arxiv_train_idx,\n",
        "        'valid': arxiv_valid_idx,\n",
        "        'test': arxiv_test_idx,\n",
        "    }\n",
        "\n",
        "    # 4. 将数据移到 GPU（如果可用）\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    features = features.to(device)\n",
        "    labels = labels.to(device)\n",
        "    arxiv_graph = arxiv_graph.to(device)\n",
        "\n",
        "    # 5. 初始化模型和优化器\n",
        "    in_feats = features.shape[1]\n",
        "    hidden_feats = 512  # 隐藏层维度\n",
        "    out_feats = labels.max().item() + 1  # 输出类别数\n",
        "    num_layers = 3  # GraphSAGE 层数\n",
        "    dropout = 0.3\n",
        "\n",
        "    model = GraphSAGE(in_feats, hidden_feats, out_feats, num_layers, dropout).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    # 6. 训练和评估\n",
        "    for epoch in range(200):  # 示例：200 轮训练\n",
        "        loss = train(model, arxiv_graph, features, labels, split_idx['train'], optimizer)\n",
        "        train_acc, valid_acc, test_acc = evaluate(model, arxiv_graph, features, labels, split_idx)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Loss = {loss:.4f}, \"\n",
        "              f\"Train Acc = {train_acc:.4f}, Valid Acc = {valid_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "MS9akx7vVgyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Output training process"
      ],
      "metadata": {
        "id": "U_5Npb0zKQcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fifth Section:now we can train the graphsage using the 2 dataset together"
      ],
      "metadata": {
        "id": "KFr9OkOUDfQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Align feature dimension\n",
        "\n"
      ],
      "metadata": {
        "id": "mH3iaHL6ON1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import dgl\n",
        "\n",
        "# 对齐特征维度, 如果维度一样可以不用对齐\n",
        "def align_feature_dimension(graph1, graph2):\n",
        "    feat_dim1 = graph1.ndata['feat'].shape[1]\n",
        "    feat_dim2 = graph2.ndata['feat'].shape[1]\n",
        "\n",
        "    # 如果维度不相等，补齐特征 #都是128维度，所以不用对齐\n",
        "    if feat_dim1 < feat_dim2:\n",
        "        pad = torch.zeros(graph1.num_nodes(), feat_dim2 - feat_dim1).to(graph1.ndata['feat'].device)\n",
        "        graph1.ndata['feat'] = torch.cat([graph1.ndata['feat'], pad], dim=1)\n",
        "    elif feat_dim2 < feat_dim1:\n",
        "        pad = torch.zeros(graph2.num_nodes(), feat_dim1 - feat_dim2).to(graph2.ndata['feat'].device)\n",
        "        graph2.ndata['feat'] = torch.cat([graph2.ndata['feat'], pad], dim=1)\n",
        "\n",
        "    return graph1, graph2\n",
        "\n",
        "#sampled_graph, arxiv_graph = align_feature_dimension(sampled_graph, arxiv_graph)\n",
        "#print(sampled_graph.ndata['feat'].shape)\n",
        "#print(arxiv_graph.ndata['feat'].shape)"
      ],
      "metadata": {
        "id": "zvH08AgbqmZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Add label to 2 graph, add ID to arxiv_graph, for sampled_graph, there has been ID."
      ],
      "metadata": {
        "id": "3opo9h7JOVR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#由于前面处理了label数据，现在需要判断前面是否处理过，如果没处理过则进行处理 如果运行了这个再运行前面的训练代码会报错，因为label已经被处理过了\n",
        "# 检查 arxiv_graph 中是否存在 'label' 数据，如果不存在就赋值\n",
        "if 'label' not in arxiv_graph.ndata or arxiv_graph.ndata['label'] is None:\n",
        "    arxiv_graph.ndata['label'] = arxiv_label\n",
        "\n",
        "# 检查 sampled_graph 中是否存在 'y' 数据，如果不存在就赋值\n",
        "if 'y' not in sampled_graph.ndata or sampled_graph.ndata['y'] is None:\n",
        "    sampled_graph.ndata['y'] = sampled_graph.ndata['y']\n",
        "\n",
        "\n",
        "# 检查 arxiv_label 是否是一维向量\n",
        "if arxiv_label.dim() > 1:\n",
        "    arxiv_graph.ndata['label'] = arxiv_label.squeeze(1)\n",
        "\n",
        "# 检查 sampled_graph 的 y 是否是一维向量\n",
        "if sampled_graph.ndata['y'].dim() > 1:\n",
        "    sampled_graph.ndata['y'] = sampled_graph.ndata['y'].squeeze(1)\n",
        "\n",
        "print(arxiv_graph.ndata['label'])\n",
        "print(sampled_graph.ndata['y'])"
      ],
      "metadata": {
        "id": "5NP_xCFoCYio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 为 arxiv_graph 添加原始 ID\n",
        "arxiv_graph.ndata['_ID'] = torch.arange(arxiv_graph.num_nodes(), dtype=torch.int64)\n",
        "\n",
        "# 验证 _ID 是否添加成功\n",
        "print(\"Node IDs (_ID):\", arxiv_graph.ndata['_ID'][:20])  # 打印前10个节点的 ID\n",
        "\n",
        "print(sampled_graph.ndata['_ID'][:20])"
      ],
      "metadata": {
        "id": "G9QB5RIxQf7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 merge 2 graph, node and edge;\n",
        "\n",
        "1)store attritute \"source\" to identity the source of the node； (Graph1 ：sampld_graph节点来源标记为 0; Graph2 ：arxiv_graph节点来源标记为 1. )\n",
        "\n",
        "2)store sttribute \"_ID\" to reserve the nodeID for mapping to the paperId.\n",
        "\n"
      ],
      "metadata": {
        "id": "1JcLTUtsPrCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sampled_graph)\n",
        "print(arxiv_graph)"
      ],
      "metadata": {
        "id": "yWASmwJkWUby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 合并两个图\n",
        "def merge_graphs(graph1, graph2):\n",
        "    import torch\n",
        "    # 合并节点特征\n",
        "    #cat中的dim参数表示第0维合并，第1维，128维度保持不变\n",
        "    print(graph1.ndata['feat'].shape)\n",
        "    print(graph2.ndata['feat'].shape)\n",
        "    combined_feat = torch.cat([graph1.ndata['feat'], graph2.ndata['feat']], dim=0)\n",
        "    print(combined_feat.shape)\n",
        "\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    def combine_and_one_hot_encode(graph1, graph2):\n",
        "        # 获取标签\n",
        "        labels1 = graph1.ndata['y']\n",
        "        labels2 = graph2.ndata['label']\n",
        "\n",
        "        # 合并标签，将两个图的标签范围扩展为 [0, total_classes)\n",
        "        total_classes = labels1.max().item() + labels2.max().item() + 2  # 总类别数为两个图的最大类别+1\n",
        "        print(labels1)\n",
        "        print(labels2)\n",
        "        combined_labels = torch.cat([labels1, labels2], dim=0)\n",
        "        print(combined_labels)\n",
        "\n",
        "        # 进行 one-hot 编码\n",
        "        one_hot_encoded_labels = F.one_hot(combined_labels, num_classes=total_classes)\n",
        "        print(one_hot_encoded_labels[-1])\n",
        "\n",
        "        return one_hot_encoded_labels\n",
        "\n",
        "    # 合并并进行编码\n",
        "    combined_labels = combine_and_one_hot_encode(graph1, graph2)\n",
        "    print(\"One-hot Encoded Labels:\")\n",
        "    print(combined_labels[3])\n",
        "\n",
        "    # 合并年份信息\n",
        "    combined_year = torch.cat([graph1.ndata['year'], graph2.ndata['year']], dim=0)\n",
        "\n",
        "    # 合并原始节点 ID\n",
        "    combined_id = torch.cat([graph1.ndata['_ID'], graph2.ndata['_ID']], dim=0)\n",
        "\n",
        "    #添加来源标签\n",
        "    graph1_source = torch.zeros(graph1.num_nodes(), dtype=torch.int64)  # Graph1 ：sampld_graph节点来源标记为 0\n",
        "    graph2_source = torch.ones(graph2.num_nodes(), dtype=torch.int64)   # Graph2 ：arxiv_graph节点来源标记为 1\n",
        "    combined_source = torch.cat([graph1_source, graph2_source], dim=0)\n",
        "\n",
        "\n",
        "    # 合并标签\n",
        "    # 合并边信息\n",
        "    graph1_edges = graph1.edges()\n",
        "    graph2_edges = graph2.edges()\n",
        "\n",
        "    # 合并边 这里对graph1的起始点做了位置偏移，所以合并后的边没有重合的\n",
        "    combined_src = torch.cat([graph1_edges[0], graph2_edges[0] + graph1.num_nodes()])\n",
        "    combined_dst = torch.cat([graph1_edges[1], graph2_edges[1] + graph1.num_nodes()])\n",
        "\n",
        "\n",
        "    # 创建新图\n",
        "    combined_graph = dgl.graph((combined_src, combined_dst))\n",
        "    combined_graph.ndata['feat'] = combined_feat\n",
        "    combined_graph.ndata['label'] = combined_labels\n",
        "    combined_graph.ndata['graph_source'] = combined_source  # 添加来源标签\n",
        "    combined_graph.ndata['year'] = combined_year\n",
        "    combined_graph.ndata['_ID'] = combined_id\n",
        "\n",
        "    print(\"Graph 1 edges:\", graph1.num_edges())\n",
        "    print(\"Graph 2 edges:\", graph2.num_edges())\n",
        "    print(\"Combined graph edges:\", combined_graph.num_edges())\n",
        "    print(\"Combined graph nodes:\", combined_graph.num_nodes())\n",
        "    print(\"Sample of graph_source:\", combined_graph.ndata['graph_source'][:10])\n",
        "    print(\"Sample of years:\", combined_graph.ndata['year'][:10])\n",
        "    print(\"Sample of _ID:\", combined_graph.ndata['_ID'][:10])\n",
        "\n",
        "    return combined_graph\n",
        "\n",
        "combined_graph = merge_graphs(sampled_graph, arxiv_graph)"
      ],
      "metadata": {
        "id": "Vv7DLZB8t4ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_graph.ndata[\"_ID\"].shape)\n",
        "print(combined_graph.ndata[\"graph_source\"].shape)"
      ],
      "metadata": {
        "id": "1E62hzEOTpXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def access_original_node(combined_graph, node_id, sampled_graph, arxiv_graph):\n",
        "    # 确定节点来源\n",
        "    print(\"hi\")\n",
        "    graph_source = combined_graph.ndata['graph_source'][node_id].item()  # 来源标识\n",
        "    print(graph_source)\n",
        "    print(combined_graph.ndata)\n",
        "    original_id = combined_graph.ndata['_ID'][node_id] # 原始 ID\n",
        "    print(original_id)\n",
        "\n",
        "    if graph_source == 0:\n",
        "        print(f\"Node {node_id} comes from 'sampled_graph', original ID: {original_id}\")\n",
        "\n",
        "    elif graph_source == 1:\n",
        "        print(f\"Node {node_id} comes from 'arxiv_graph', original ID: {original_id}\")\n",
        "\n",
        "\n",
        "# 访问第 0 个节点的数据\n",
        "access_original_node(combined_graph, node_id=2, sampled_graph=sampled_graph, arxiv_graph=arxiv_graph)\n",
        "\n",
        "# 访问第 200,000 个节点的数据\n",
        "access_original_node(combined_graph, node_id=200003, sampled_graph=sampled_graph, arxiv_graph=arxiv_graph)\n"
      ],
      "metadata": {
        "id": "VvU0phH4UOkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Add the masks attribute to the combined graph\n",
        "\n",
        "remeber the order id of the node in the combined graph can't be seen as the masking id, when we are doing the mask id, always think about the _ID attribute.\n",
        "\n",
        "One way to check this is to output the year of the train node, in arxiv dataset, they are less than 2018, while in the sampled dataset, they are less than 2013.\n",
        "\n"
      ],
      "metadata": {
        "id": "e2SsfbqSaoHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph1 = sampled_graph\n",
        "graph2 = arxiv_graph\n",
        "#存储原始状态"
      ],
      "metadata": {
        "id": "tVGCW5ctfpJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_and_initialize_masks(sampled_graph, arxiv_graph):\n",
        "    # 处理 sampled_graph\n",
        "    if 'train_mask' not in sampled_graph.ndata or 'valid_mask' not in sampled_graph.ndata or 'test_mask' not in sampled_graph.ndata:\n",
        "        print(\"Initializing masks for sampled_graph...\")\n",
        "        years = sampled_graph.ndata['year'].flatten()  # 获取年份信息\n",
        "        print(years)\n",
        "        train_cutoff_year = 2013\n",
        "        valid_cutoff_year = 2015\n",
        "\n",
        "        # 划分训练集、验证集和测试集\n",
        "        sampled_graph = split_by_year(sampled_graph, years, train_cutoff_year, valid_cutoff_year)\n",
        "\n",
        "        # 验证划分结果\n",
        "        print(\"Number of training nodes:\", sampled_graph.ndata['train_mask'].sum().item())\n",
        "        print(\"Number of validation nodes:\", sampled_graph.ndata['valid_mask'].sum().item())\n",
        "        print(\"Number of testing nodes:\", sampled_graph.ndata['test_mask'].sum().item())\n",
        "    else:\n",
        "        print(\"Masks for sampled_graph already initialized!\")\n",
        "\n",
        "    # 处理 arxiv_graph\n",
        "    if 'train_mask' not in arxiv_graph.ndata or 'valid_mask' not in arxiv_graph.ndata or 'test_mask' not in arxiv_graph.ndata:\n",
        "        print(\"Initializing masks for arxiv_graph...\")\n",
        "        # 加载 ogbn-arxiv 数据集\n",
        "        arxiv_dataset = DglNodePropPredDataset(name=\"ogbn-arxiv\")\n",
        "        arxiv_split_idx = arxiv_dataset.get_idx_split()\n",
        "        arxiv_train_idx, arxiv_valid_idx, arxiv_test_idx = (\n",
        "            arxiv_split_idx[\"train\"], arxiv_split_idx[\"valid\"], arxiv_split_idx[\"test\"]\n",
        "        )\n",
        "        #arxiv_graph.ndata['label'] = arxiv_graph.ndata['label'].squeeze(1)  # 确保标签是一维向量\n",
        "\n",
        "        # 设置掩码\n",
        "        arxiv_graph.ndata['train_mask'] = torch.zeros(arxiv_graph.num_nodes(), dtype=torch.bool)\n",
        "        arxiv_graph.ndata['valid_mask'] = torch.zeros(arxiv_graph.num_nodes(), dtype=torch.bool)\n",
        "        arxiv_graph.ndata['test_mask'] = torch.zeros(arxiv_graph.num_nodes(), dtype=torch.bool)\n",
        "\n",
        "        print(arxiv_train_idx)\n",
        "\n",
        "        # 根据划分索引设置掩码\n",
        "        arxiv_graph.ndata['train_mask'][arxiv_train_idx] = True #在对应的点设为mask， 这里的idx是直接的索引\n",
        "        arxiv_graph.ndata['valid_mask'][arxiv_valid_idx] = True\n",
        "        arxiv_graph.ndata['test_mask'][arxiv_test_idx] = True\n",
        "\n",
        "        print(arxiv_graph.ndata['train_mask'])\n",
        "\n",
        "        # 验证划分结果\n",
        "        print(\"Number of training nodes:\", arxiv_graph.ndata['train_mask'].sum().item())\n",
        "        print(\"Number of validation nodes:\", arxiv_graph.ndata['valid_mask'].sum().item())\n",
        "        print(\"Number of testing nodes:\", arxiv_graph.ndata['test_mask'].sum().item())\n",
        "    else:\n",
        "        print(\"Masks for arxiv_graph already initialized!\")\n",
        "\n",
        "    return sampled_graph, arxiv_graph\n",
        "\n",
        "# 使用分割函数\n",
        "def split_by_year(graph, years, train_cutoff, valid_cutoff):\n",
        "    # 创建布尔掩码\n",
        "    train_mask = (years <= train_cutoff)  # 截止到 train_cutoff 年份属于训练集\n",
        "    valid_mask = (years > train_cutoff) & (years <= valid_cutoff)  # 在 valid_cutoff 年份之间属于验证集\n",
        "    test_mask = (years > valid_cutoff)  # 剩余的属于测试集\n",
        "\n",
        "    print(train_mask)\n",
        "    print(valid_mask)\n",
        "    print(test_mask)\n",
        "\n",
        "    # 将掩码添加到图中\n",
        "    graph.ndata['train_mask'] = train_mask\n",
        "    graph.ndata['valid_mask'] = valid_mask\n",
        "    graph.ndata['test_mask'] = test_mask\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KvwJru0OdZeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_graph, arxiv_graph = check_and_initialize_masks(sampled_graph,arxiv_graph)"
      ],
      "metadata": {
        "id": "JKAArfNqf2vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sampled_graph = graph1\n",
        "#arxiv_graph = graph2"
      ],
      "metadata": {
        "id": "E85kqMkCgmFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sampled_graph)\n",
        "print(arxiv_graph)"
      ],
      "metadata": {
        "id": "4Is6h2G8ed8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 合并掩码\n",
        "def merge_masks(graph1, graph2):\n",
        "\n",
        "    train_mask = torch.cat([graph1.ndata['train_mask'], graph2.ndata['train_mask']], dim=0)\n",
        "    valid_mask = torch.cat([graph1.ndata['valid_mask'], graph2.ndata['valid_mask']], dim=0)\n",
        "    test_mask = torch.cat([graph1.ndata['test_mask'], graph2.ndata['test_mask']], dim=0)\n",
        "\n",
        "    return train_mask, valid_mask, test_mask\n",
        "\n",
        "# 合并掩码\n",
        "combined_train_mask, combined_valid_mask, combined_test_mask = merge_masks(sampled_graph, arxiv_graph)"
      ],
      "metadata": {
        "id": "Hgb_FPZXt5j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_train_mask[-2000:])\n",
        "\n",
        "print(arxiv_graph.ndata['train_mask'][:100]) # for arxiv , the paper is sorted by year already, so the front are training , the last are testing\n",
        "\n",
        "print(combined_train_mask[:10])\n",
        "print(sampled_graph.ndata['train_mask'][:10])"
      ],
      "metadata": {
        "id": "SZpYJXZSeLUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 对齐特征维度 如果维度一样可以不用对齐\n",
        "#sampled_graph, arxiv_graph = align_feature_dimension(sampled_graph, arxiv_graph)\n",
        "\n",
        "# 合并两个图\n",
        "#combined_graph = merge_graphs(sampled_graph, arxiv_graph)\n",
        "\n",
        "# 合并掩码\n",
        "#combined_train_mask, combined_valid_mask, combined_test_mask = merge_masks(sampled_graph, arxiv_graph)\n",
        "\n",
        "\n",
        "# 添加合并后的掩码到图中\n",
        "combined_graph.ndata['train_mask'] = combined_train_mask\n",
        "combined_graph.ndata['valid_mask'] = combined_valid_mask\n",
        "combined_graph.ndata['test_mask'] = combined_test_mask\n",
        "\n",
        "# 验证结果\n",
        "print(\"Number of nodes in combined graph:\", combined_graph.num_nodes())\n",
        "print(\"Number of edges in combined graph:\", combined_graph.num_edges())\n",
        "print(\"Number of training nodes:\", combined_train_mask.sum().item())\n",
        "print(\"Number of validation nodes:\", combined_valid_mask.sum().item())\n",
        "print(\"Number of testing nodes:\", combined_test_mask.sum().item())\n"
      ],
      "metadata": {
        "id": "ItV8KI6Nt7aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 save and load the graph"
      ],
      "metadata": {
        "id": "IuvsCZwZjx17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_graph)\n",
        "import pickle\n",
        "\n",
        "def save_graph(graph, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(graph, f)\n",
        "    print(f\"Graph saved to {filename}\")\n",
        "\n",
        "# 保存图并压缩\n",
        "save_graph(combined_graph, \"combined_graph.pkl\")"
      ],
      "metadata": {
        "id": "vqzARe4HKsQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "def save_graph_compressed(graph, filename):\n",
        "    # 使用 gzip 压缩保存图\n",
        "    with gzip.open(filename, 'wb') as f:\n",
        "        pickle.dump(graph, f)\n",
        "    print(f\"Graph saved and compressed to {filename}\")\n",
        "\n",
        "def load_graph_compressed(filename):\n",
        "    # 使用 gzip 解压缩并加载图\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        graph = pickle.load(f)\n",
        "    print(f\"Graph loaded from {filename}\")\n",
        "    return graph\n",
        "# 压缩保存图\n",
        "save_graph_compressed(combined_graph, \"combined_graph.pkl.gz\")"
      ],
      "metadata": {
        "id": "BgJY80JlLpSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载压缩图\n",
        "combined_graph = load_graph_compressed(\"combined_graph.pkl.gz\")\n",
        "\n",
        "# 验证加载后的图结构\n",
        "print(\"Node data:\", combined_graph.ndata.keys())\n",
        "print(\"Edge data:\", combined_graph.edata.keys())"
      ],
      "metadata": {
        "id": "tXEEAfuKL8dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_graph)"
      ],
      "metadata": {
        "id": "PF6HzyJXfn2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6 Finally train it!"
      ],
      "metadata": {
        "id": "FsrGO-eQmFev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(arxiv_label)"
      ],
      "metadata": {
        "id": "T93kUiqmo8pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This one is for cpu\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "from dgl.nn import SAGEConv\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#This below is the same with the function in section 3\n",
        "# 定义 GraphSAGE 模型\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats, out_feats, num_layers, dropout):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        self.layers.append(SAGEConv(in_feats, hidden_feats, aggregator_type='mean'))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.layers.append(SAGEConv(hidden_feats, hidden_feats, aggregator_type='mean'))\n",
        "        self.layers.append(SAGEConv(hidden_feats, out_feats, aggregator_type='mean'))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(g, x)\n",
        "            if i != len(self.layers) - 1:  # 最后一层不使用激活\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "# 训练函数\n",
        "def train(model, graph, features, labels, train_idx, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    labels = labels.float()\n",
        "    out = model(graph, features)\n",
        "    loss = F.cross_entropy(out[train_idx], labels[train_idx], label_smoothing=0.1)  # 添加标签平滑\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# 验证/测试函数\n",
        "@torch.no_grad()\n",
        "def evaluate(model, graph, features, labels, split_idx):\n",
        "    model.eval()\n",
        "    out = model(graph, features)\n",
        "    preds = out.argmax(dim=1)\n",
        "\n",
        "    # 转换标签为类别索引形式\n",
        "    labels_idx = labels.argmax(dim=1)  # 从 one-hot 编码转为索引形式\n",
        "\n",
        "    train_acc = accuracy_score(labels_idx[split_idx['train']].cpu(), preds[split_idx['train']].cpu())\n",
        "    valid_acc = accuracy_score(labels_idx[split_idx['valid']].cpu(), preds[split_idx['valid']].cpu())\n",
        "    test_acc = accuracy_score(labels_idx[split_idx['test']].cpu(), preds[split_idx['test']].cpu())\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ],
      "metadata": {
        "id": "qqxPWXGcntxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import dgl\n",
        "import os\n",
        "\n",
        "def batchify(graph, train_idx, batch_size):\n",
        "    # 将训练节点划分为批次\n",
        "    for i in range(0, len(train_idx), batch_size):\n",
        "        batch_nodes = train_idx[i:i + batch_size]\n",
        "        subgraph = dgl.node_subgraph(graph, batch_nodes)  # 创建子图\n",
        "        yield subgraph, batch_nodes\n",
        "\n",
        "\n",
        "# 提取特征和标签\n",
        "features = combined_graph.ndata['feat']\n",
        "labels = combined_graph.ndata['label']\n",
        "train_idx = torch.nonzero(combined_graph.ndata['train_mask'], as_tuple=True)[0]\n",
        "valid_idx = torch.nonzero(combined_graph.ndata['valid_mask'], as_tuple=True)[0]\n",
        "test_idx = torch.nonzero(combined_graph.ndata['test_mask'], as_tuple=True)[0]\n",
        "\n",
        "split_idx = {'train': train_idx, 'valid': valid_idx, 'test': test_idx}\n",
        "\n",
        "# 将数据移动到合适的设备\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "features = features.to(device)\n",
        "labels = labels.to(device)\n",
        "combined_graph = combined_graph.to(device)\n",
        "\n",
        "# 初始化模型和优化器\n",
        "in_feats = features.shape[1]\n",
        "hidden_feats = 512\n",
        "out_feats = labels.shape[1]  # 输出类别数 (来自 one-hot 编码)\n",
        "num_layers = 3\n",
        "dropout = 0.3\n",
        "\n",
        "model = GraphSAGE(in_feats, hidden_feats, out_feats, num_layers, dropout).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "# 创建保存路径\n",
        "save_dir = \"./saved_models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 定义批次大小\n",
        "batch_size = 1024\n",
        "\n",
        "best_valid_acc = 0\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # 手动处理每个批次\n",
        "    for subgraph, batch_nodes in batchify(combined_graph, train_idx, batch_size):\n",
        "        subgraph = subgraph.to(device)  # 将子图移动到设备\n",
        "        batch_feats = subgraph.ndata['feat']\n",
        "        batch_labels = subgraph.ndata['label']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(subgraph, batch_feats)\n",
        "        loss = -(batch_labels * torch.log_softmax(logits, dim=1)).sum(dim=1).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # 验证和测试\n",
        "    train_acc, valid_acc, test_acc = evaluate(model, combined_graph, features, labels,\n",
        "                                              {'train': train_idx, 'valid': valid_idx, 'test': test_idx})\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}: Loss = {total_loss:.4f}, \"\n",
        "          f\"Train Acc = {train_acc:.4f}, Valid Acc = {valid_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
        "\n",
        "    # 保存验证集上最好的模型\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_model_path = os.path.join(save_dir, \"best_model.pth\")\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved at epoch {epoch + 1}: {best_model_path}\")\n",
        "\n",
        "    # 每 50 个 epoch 保存一次模型\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        model_save_path = os.path.join(save_dir, f\"model_epoch_{epoch + 1}.pth\")\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Model saved at epoch {epoch + 1}: {model_save_path}\")"
      ],
      "metadata": {
        "id": "dSp20_SoqkPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 清理垃圾收集器\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "FcEw3zqZuqcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other session, may used when using graphalign embedding"
      ],
      "metadata": {
        "id": "l17cqR6prIV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#现在要用e5编码我的paper了，如果我还是用标题和摘要呢？\n",
        "\n",
        "# 加载你生成的 E5 编码（假设形状为 [num_papers, 384]）\n",
        "paper_embeddings = np.load('paper_embeddings.npy')\n",
        "paper_embeddings = torch.tensor(paper_embeddings, dtype=torch.float32)\n",
        "graph.nodes['paper'].data['feat'] = paper_embeddings\n",
        "\n",
        "# 为 author 节点生成随机初始特征（可替换为其他方法）??不要随机初始特征吧\n",
        "graph.nodes['author'].data['feat'] = torch.randn(graph.num_nodes('author'), 128)\n",
        "graph.nodes['institution'].data['feat'] = torch.randn(graph.num_nodes('institution'), 128)\n",
        "graph.nodes['field_of_study'].data['feat'] = torch.randn(graph.num_nodes('field_of_study'), 128)\n",
        "\n",
        "\"\"\"加入不使用随机初始化\n",
        "# 方法1：作者特征 = 其撰写论文的 E5 特征均值\n",
        "# ---------------------------------------------------\n",
        "# 获取作者与论文的边关系（'author' -- 'writes' --> 'paper'）\n",
        "author_paper_edges = graph.edges(etype=('author', 'writes', 'paper'))\n",
        "author_ids, paper_ids = author_paper_edges[0], author_paper_edges[1]\n",
        "\n",
        "# 计算每个作者的特征（聚合其所有论文的 E5 嵌入）\n",
        "author_feat = torch.zeros(graph.num_nodes('author'), paper_embeddings.shape[1])\n",
        "for author_id in torch.unique(author_ids):\n",
        "    mask = author_ids == author_id\n",
        "    author_feat[author_id] = paper_embeddings[paper_ids[mask]].mean(dim=0)\n",
        "\n",
        "graph.nodes['author'].data['feat'] = author_feat\n",
        "\n",
        "# 方法2：机构特征 = 其下属作者的特征均值\n",
        "# ---------------------------------------------------\n",
        "# 获取机构与作者的边关系（'author' -- 'affiliated_with' --> 'institution'）\n",
        "author_inst_edges = graph.edges(etype=('author', 'affiliated_with', 'institution'))\n",
        "author_ids, inst_ids = author_inst_edges[0], author_inst_edges[1]\n",
        "\n",
        "# 计算每个机构的特征（聚合其所有作者的特征）\n",
        "institution_feat = torch.zeros(graph.num_nodes('institution'), author_feat.shape[1])\n",
        "for inst_id in torch.unique(inst_ids):\n",
        "    mask = inst_ids == inst_id\n",
        "    institution_feat[inst_id] = author_feat[author_ids[mask]].mean(dim=0)\n",
        "\n",
        "graph.nodes['institution'].data['feat'] = institution_feat\n",
        "\n",
        "# 方法3：研究领域特征 = 其相关论文的 E5 特征均值\n",
        "# ---------------------------------------------------\n",
        "# 获取论文与研究领域的边关系（'paper' -- 'has_topic' --> 'field_of_study'）\n",
        "paper_field_edges = graph.edges(etype=('paper', 'has_topic', 'field_of_study'))\n",
        "paper_ids, field_ids = paper_field_edges[0], paper_field_edges[1]\n",
        "\n",
        "# 计算每个领域的特征（聚合其相关论文的 E5 嵌入）\n",
        "field_feat = torch.zeros(graph.num_nodes('field_of_study'), paper_embeddings.shape[1])\n",
        "for field_id in torch.unique(field_ids):\n",
        "    mask = field_ids == field_id\n",
        "    field_feat[field_id] = paper_embeddings[paper_ids[mask]].mean(dim=0)\n",
        "\n",
        "graph.nodes['field_of_study'].data['feat'] = field_feat\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Gnlan90E2oOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "paper_mapping = pd.read_csv(\"/content/dataset/ogbn_mag/mapping/paper_entidx2name.csv.gz\", compression = \"gzip\")\n",
        "print(paper_mapping) #这是nodeid到mag papaer id的映射"
      ],
      "metadata": {
        "id": "a2eDSg4JytVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "author_mapping = pd.read_csv(\"/content/dataset/ogbn_mag/mapping/author_entidx2name.csv.gz\", compression = \"gzip\")\n",
        "print(author_mapping) #这是nodeid到author id的映射"
      ],
      "metadata": {
        "id": "7wzlji6b0qXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(graph,label)"
      ],
      "metadata": {
        "id": "kQ0SS29L8XFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GraphSAGE是为同构图设计的，无法直接处理异构图的多类型节点和边\n",
        "#分类型单独应用GraphSAGE，再聚合结果\n",
        "#调整后的GraphSAGE模型\n",
        "from dgl.nn import SAGEConv\n",
        "import torch.nn as nn\n",
        "\n",
        "class HeteroGraphSAGE(nn.Module):\n",
        "    def __init__(self, in_feats_dict, hid_feats, out_feats):\n",
        "        super().__init__()\n",
        "        # 定义每个节点类型的投影层\n",
        "        self.proj = nn.ModuleDict({\n",
        "            ntype: nn.Linear(in_feats_dict[ntype], hid_feats)\n",
        "            for ntype in in_feats_dict\n",
        "        })\n",
        "        # 定义异构卷积层（分边类型处理）\n",
        "        self.conv1 = dgl.nn.HeteroGraphConv({\n",
        "            rel: SAGEConv(hid_feats, hid_feats, 'mean')\n",
        "            for rel in graph.etypes\n",
        "        })\n",
        "        self.classifier = nn.Linear(hid_feats, out_feats)\n",
        "\n",
        "    def forward(self, graph):\n",
        "        # 投影所有节点特征到 hid_feats\n",
        "        h_dict = {ntype: self.proj[ntype](graph.nodes[ntype].data['feat'])\n",
        "                  for ntype in graph.ntypes}\n",
        "        # 异构卷积\n",
        "        h_dict = self.conv1(graph, h_dict)\n",
        "        h_dict = {k: nn.functional.relu(v) for k, v in h_dict.items()}\n",
        "        # 分类（仅 paper 节点）\n",
        "        return self.classifier(h_dict['paper'])"
      ],
      "metadata": {
        "id": "tUj683sA-bxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#训练与苹"
      ],
      "metadata": {
        "id": "D8Jx6jrQCPLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#用不到，这是用来保存图的\n",
        "import dgl\n",
        "import torch\n",
        "import pickle\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "\n",
        "# 加载数据\n",
        "dataset = DglNodePropPredDataset(name=\"ogbn-arxiv\")\n",
        "split_idx = dataset.get_idx_split()\n",
        "graph, labels = dataset[0]\n",
        "\n",
        "# 方法1：保存为pickle\n",
        "data_to_save = {\"graph\": graph, \"labels\": labels, \"split_idx\": split_idx}\n",
        "with open(\"arxiv_data.pkl\", \"wb\") as f:\n",
        "    pickle.dump(data_to_save, f)\n",
        "\n",
        "# 方法2：使用DGL内置函数\n",
        "dgl.save_graphs(\"arxiv_graph.dgl\", [graph])\n",
        "torch.save({\"labels\": labels, \"split_idx\": split_idx}, \"arxiv_meta.pt\")\n",
        "\n",
        "print(\"数据保存成功！\")"
      ],
      "metadata": {
        "id": "Dz2CNe5S630I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Kdt5G92lwOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "id": "NqkBjfan9Mvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluator\n",
        "from ogb.graphproppred import Evaluator\n",
        "\n",
        "evaluator = Evaluator(name = \"ogbg-mag\")\n",
        "# You can learn the input and output format specification of the evaluator as follows.\n",
        "# print(evaluator.expected_input_format)\n",
        "# print(evaluator.expected_output_format)\n",
        "input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "result_dict = evaluator.eval(input_dict) # E.g., {\"rocauc\": 0.7321}"
      ],
      "metadata": {
        "id": "_0jg6ASAgZkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#测试embedding，但是跑不起来\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "import wget\n",
        "import shutil\n",
        "import gzip\n",
        "\n",
        "\n",
        "MODEL_NAME = {\"e5\": \"intfloat/e5-small-v2\", \"ofa\": \"../../cache/transformer-model/multi-qa-distilbert-cos-v1\"}\n",
        "short_name = {\"ogbn-papers100M\": \"100M\", \"ogbn-arxiv\": \"arxiv\", \"ogbn-products\": \"products\"}\n",
        "\n",
        "\n",
        "def decompress_gz(file_path, output_path):\n",
        "    with gzip.open(file_path, 'rb') as f_in:\n",
        "        with open(output_path, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    print(f\"Decompressed file to {output_path}\")\n",
        "\n",
        "\n",
        "class Ogb_dataset(Dataset):\n",
        "    def __init__(self, datas):\n",
        "        self.data = datas\n",
        "        self.length = len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "\n",
        "class Tokenizer(object):\n",
        "    def __init__(self, tokenizer, args):\n",
        "        super(Tokenizer, self).__init__()\n",
        "        self.max_token_len = args.max_token_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.padding = \"max_length\"\n",
        "        self.truncation = True\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        if isinstance(examples, str):\n",
        "            return self.tokenizer(examples, padding=self.padding, truncation=self.truncation,\n",
        "                                  max_length=self.max_token_len, return_tensors=\"pt\")\n",
        "        else:\n",
        "            return self.tokenizer(examples[\"text\"], padding=self.padding, truncation=self.truncation,\n",
        "                                  max_length=self.max_token_len, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "class Gen_ogb_data():\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.download_raw_data(args.dataset_name)\n",
        "        self.get_text_data(args.dataset_name)\n",
        "        self.get_emb(args.dataset_name)\n",
        "\n",
        "    def average_pool(self, last_hidden_states, attention_mask):\n",
        "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "    def download_raw_data(self, dataset_name=\"ogbn-arxiv\"):\n",
        "        os.makedirs(self.args.data_save_path, exist_ok=True)\n",
        "        dataset_path = os.path.join(self.args.data_save_path, dataset_name)\n",
        "        os.makedirs(dataset_path, exist_ok=True)\n",
        "        if dataset_name == \"ogbn-arxiv\":\n",
        "            if not os.path.exists(os.path.join(dataset_path, \"titleabs.tsv\")):\n",
        "                url = \"https://snap.stanford.edu/ogb/data/misc/ogbn_arxiv/titleabs.tsv.gz\"\n",
        "                wget.download(url, os.path.join(dataset_path, \"titleabs.tsv.gz\"))\n",
        "                decompress_gz(os.path.join(dataset_path, \"titleabs.tsv.gz\"), os.path.join(dataset_path, \"titleabs.tsv\"))\n",
        "                os.remove(os.path.join(dataset_path, \"titleabs.tsv.gz\"))\n",
        "            self.dgl_dataset = DglNodePropPredDataset(dataset_name, root=os.path.join(self.args.data_save_path,\n",
        "                                                                                      \"ogb-official-data\"))\n",
        "\n",
        "    def get_text_data(self, dataset_name=\"ogbn-arxiv\"):\n",
        "        dataset_path = os.path.join(self.args.data_save_path, dataset_name)\n",
        "        ogbn_official_path = os.path.join(self.args.data_save_path, \"ogb-official-data\")\n",
        "\n",
        "        if dataset_name == \"ogbn-arxiv\":\n",
        "            self.df = pd.read_csv(os.path.join(dataset_path, 'titleabs.tsv'), sep='\\t')\n",
        "            decompress_gz(os.path.join(ogbn_official_path, \"ogbn_arxiv\", \"mapping\", 'nodeidx2paperid.csv.gz'),\n",
        "                          os.path.join(ogbn_official_path, \"ogbn_arxiv\", \"mapping\", 'nodeidx2paperid.csv'))\n",
        "            self.nodeid2contentid = pd.read_csv(\n",
        "                os.path.join(ogbn_official_path, \"ogbn_arxiv\", \"mapping\", 'nodeidx2paperid.csv'))\n",
        "            self.df.columns = [\"paperid\", \"title\", \"abs\"]\n",
        "            self.nodeid2contentid.columns = [\"nodeid\", \"paperid\"]\n",
        "            data = pd.merge(self.nodeid2contentid, self.df, how=\"left\", on=\"paperid\")\n",
        "            Datasets = data.values[:, 2:]\n",
        "\n",
        "        dataframe = pd.DataFrame(Datasets)\n",
        "        dataframe.to_csv(os.path.join(dataset_path, f'{dataset_name}_title_content.csv'), index=False)\n",
        "        print(f\"{dataset_name} title_content.csv has been saved!\")\n",
        "\n",
        "    def get_emb(self, dataset_name=\"ogbn-arxiv\"):\n",
        "        dataset_path = os.path.join(self.args.data_save_path, dataset_name)\n",
        "        if dataset_name == \"ogbn-arxiv\":\n",
        "            Datas = []\n",
        "            data = pd.read_csv(os.path.join(dataset_path, f'{dataset_name}_title_content.csv')).values\n",
        "            for k in range(data.shape[0]):\n",
        "                data_dict = {}\n",
        "                if pd.isnull(data[k][0]) and pd.isnull(data[k][1]):\n",
        "                    data_dict[\"text\"] = \" .  \"\n",
        "                elif pd.isnull(data[k][1]):\n",
        "                    data_dict[\"text\"] = data[k][0]\n",
        "                elif pd.isnull(data[k][0]):\n",
        "                    data_dict[\"text\"] = data[k][1]\n",
        "                else:\n",
        "                    data_dict[\"text\"] = data[k][0] + \". \" + data[k][1]\n",
        "                Datas.append(data_dict)\n",
        "            text_dataset = Ogb_dataset(Datas)\n",
        "            text_dataloader = DataLoader(text_dataset, shuffle=False, batch_size=self.args.batch_size)\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        if self.args.Model == \"e5\":\n",
        "            tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME[self.args.Model])\n",
        "            model_tokenizer = Tokenizer(tokenizer, self.args)\n",
        "            model = AutoModel.from_pretrained(MODEL_NAME[self.args.Model])\n",
        "            model.to(self.args.device)\n",
        "            model.eval()\n",
        "            nodes_embed = []\n",
        "            epoch_iter = tqdm(text_dataloader)\n",
        "            print(f\"Generating {self.args.Model} embedding!\")\n",
        "            with torch.no_grad():\n",
        "                for batch in epoch_iter:\n",
        "                    batch = model_tokenizer(batch)\n",
        "                    batch = {k: v.to(self.args.device) for k, v in batch.items()}\n",
        "                    outputs = model(**batch)\n",
        "                    embeddings = self.average_pool(outputs.last_hidden_state, batch['attention_mask'])\n",
        "                    for i in range(embeddings.shape[0]):\n",
        "                        nodes_embed.append(embeddings[i].cpu().numpy().astype(self.args.dtype)) #我们这没有这么复杂\n",
        "\n",
        "            nodes_embed = np.stack(nodes_embed, axis=0)\n",
        "            np.save(os.path.join(dataset_path, f\"{short_name[dataset_name]}_embedding_{self.args.Model}_{self.args.dtype}.npy\"),\n",
        "                    nodes_embed)\n",
        "            print(f\"{short_name[dataset_name]}_embedding_{self.args.Model}_{self.args.dtype}.npy has been saved!\")\n",
        "\n",
        "        elif self.args.Model == \"ofa\":\n",
        "            model = SentenceTransformer(MODEL_NAME[self.args.Model])\n",
        "            model.to(self.args.device)\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                texts = []\n",
        "                for d in Datas:\n",
        "                    texts.append(d[\"text\"])\n",
        "                embeddings = model.encode(texts, batch_size=self.args.batch_size, show_progress_bar=True,\n",
        "                                          convert_to_tensor=False, convert_to_numpy=True)\n",
        "                np.save(os.path.join(dataset_path, f\"{short_name[dataset_name]}_embedding_{self.args.Model}_{self.args.dtype}.npy\"),\n",
        "                        embeddings.astype(self.args.dtype))\n",
        "            print(f\"{short_name[dataset_name]}_embedding_{self.args.Model}_{self.args.dtype}.npy has been saved!\")\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 加载数据集\n",
        "    dataset = DglNodePropPredDataset(name=\"ogbn-arxiv\")\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "    graph, label = dataset[0]\n",
        "\n",
        "    # 设置参数\n",
        "    parser = argparse.ArgumentParser(description=\"\")\n",
        "    parser.add_argument('--data_save_path', type=str, default='./data')\n",
        "    parser.add_argument('--device', type=str, default='cuda:0')\n",
        "    parser.add_argument('--Model', type=str, default='e5')\n",
        "    parser.add_argument('--batch_size', type=int, default=512)\n",
        "    parser.add_argument('--dataset_name', type=str, default=\"ogbn-arxiv\")\n",
        "    parser.add_argument('--max_token_len', type=int, default=512)\n",
        "    parser.add_argument('--dtype', type=str, default=\"float16\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # 生成嵌入\n",
        "    gen_ogb = Gen_ogb_data(args)\n"
      ],
      "metadata": {
        "id": "vChM0dMPifgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "id": "M_M22Y0lifmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#一种很新的训练方法\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from dgl.data import AmazonCoBuyComputerDataset\n",
        "from dgl.nn import SAGEConv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "# Set the DGL backend to PyTorch\n",
        "os.environ['DGLBACKEND'] = 'pytorch'\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = AmazonCoBuyComputerDataset()\n",
        "g = dataset[0]\n",
        "labels = g.ndata['label']\n",
        "\n",
        "# Split the dataset into training/validation/testing sets\n",
        "n_nodes = g.num_nodes()\n",
        "indices = np.random.permutation(n_nodes)\n",
        "train_idx = indices[:int(0.8*n_nodes)]  # 80% for training\n",
        "val_idx = indices[int(0.8*n_nodes):int(0.9*n_nodes)]  # 10% for validation\n",
        "test_idx = indices[int(0.9*n_nodes):]  # 10% for testing\n",
        "\n",
        "# Create training/validation/testing masks\n",
        "g.ndata['train_mask'] = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "g.ndata['val_mask'] = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "g.ndata['test_mask'] = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "g.ndata['train_mask'][train_idx] = True\n",
        "g.ndata['val_mask'][val_idx] = True\n",
        "g.ndata['test_mask'][test_idx] = True\n",
        "\n",
        "# Define the GraphSAGE model\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_feats, hid_feats, out_feats):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, hid_feats, 'mean')  # First GraphSAGE layer\n",
        "        self.conv2 = SAGEConv(hid_feats, hid_feats, 'mean')  # Second GraphSAGE layer\n",
        "        self.classifier = nn.Linear(hid_feats, out_feats)  # Classification layer\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        x = F.relu(self.conv1(graph, x))  # Apply ReLU activation after the first layer\n",
        "        x = F.relu(self.conv2(graph, x))  # Apply ReLU activation after the second layer\n",
        "        return self.classifier(x)  # Output layer\n",
        "\n",
        "# Prepare three types of node features\n",
        "def prepare_features(g):\n",
        "    # Feature type (i): All-one vectors\n",
        "    ones_feat = torch.ones(g.ndata['feat'].shape)\n",
        "\n",
        "    # Feature type (ii): Original node features\n",
        "    original_feat = g.ndata['feat']\n",
        "\n",
        "    # Feature type (iii): Structural features + one-hot encoding\n",
        "    # Convert to a NetworkX graph\n",
        "    # nx_g = g.to_networkx().to_undirected() This line would throw an error for multigraphs\n",
        "    nx_g = nx.Graph(dgl.to_networkx(g.cpu()).to_undirected())\n",
        "    # Note: nx.Graph function will merge multiple edges between nodes into a single edge,\n",
        "    # as NetworkX does not support multigraph structures.\n",
        "    print(nx_g.number_of_edges())  # Should output the original number of undirected edges: 245,778\n",
        "\n",
        "    # Calculate structural features\n",
        "    print(\"Calculating clustering coefficients...\")\n",
        "    clustering = nx.clustering(nx_g)\n",
        "    print(\"Calculating degree centrality...\")\n",
        "    degree_cent = nx.degree_centrality(nx_g)\n",
        "    print(\"Calculating betweenness centrality...\")\n",
        "    betweenness = nx.betweenness_centrality(nx_g, k=100)  # Sample some nodes to speed up calculation\n",
        "    print(\"Calculating eigenvector centrality...\")\n",
        "    eigenvector = nx.eigenvector_centrality(nx_g, max_iter=1000)\n",
        "\n",
        "    # Collect features and normalize them\n",
        "    structural = np.array([[clustering[i], degree_cent[i],\n",
        "                          betweenness[i], eigenvector[i]] for i in range(n_nodes)])\n",
        "    structural = StandardScaler().fit_transform(structural)\n",
        "\n",
        "    # Generate one-hot encodings\n",
        "    one_hot = torch.eye(n_nodes)\n",
        "\n",
        "    # Concatenate features\n",
        "    structural_feat = torch.cat([torch.FloatTensor(structural), one_hot], dim=1)\n",
        "\n",
        "    return {\n",
        "        'ones': ones_feat,\n",
        "        'original': original_feat,\n",
        "        'structural+onehot': structural_feat\n",
        "    }\n",
        "\n",
        "features = prepare_features(g)\n",
        "\n",
        "# Define the training function\n",
        "def train_model(feature, label, train_mask, val_mask, test_mask, epochs=400):\n",
        "    model = GraphSAGE(feature.shape[1], 64, dataset.num_classes)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer with learning rate 0.01\n",
        "    criterion = nn.CrossEntropyLoss()  # Cross-entropy loss function\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_model = None\n",
        "    history = {'loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        logits = model(g, feature)\n",
        "        loss = criterion(logits[train_mask], label[train_mask])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(1)\n",
        "            val_acc = accuracy_score(label[val_mask].numpy(), pred[val_mask].numpy())\n",
        "\n",
        "        history['loss'].append(loss.item())\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model = model.state_dict().copy()\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f'Epoch {epoch:03d} | Loss: {loss.item():.4f} | Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    # Test the best model\n",
        "    model.load_state_dict(best_model)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(g, feature)\n",
        "        test_acc = accuracy_score(label[test_mask].numpy(),\n",
        "                                 logits[test_mask].argmax(1).numpy())\n",
        "\n",
        "    return history, test_acc\n",
        "\n",
        "# Train and compare three types of features\n",
        "results = {}\n",
        "for feat_name in ['ones', 'original', 'structural+onehot']:\n",
        "    print(f\"\\n=== Training with {feat_name} features ===\")\n",
        "    feat = features[feat_name]\n",
        "    history, test_acc = train_model(feat, labels,\n",
        "                                  g.ndata['train_mask'],\n",
        "                                  g.ndata['val_mask'],\n",
        "                                  g.ndata['test_mask'])\n",
        "    results[feat_name] = {\n",
        "        'train_loss': history['loss'],\n",
        "        'val_acc': history['val_acc'],\n",
        "        'test_acc': test_acc\n",
        "    }\n",
        "\n",
        "# Visualize the training process\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, feat in enumerate(results.keys()):\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(results[feat]['train_loss'], label=f'{feat}')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(results[feat]['val_acc'], label=f'{feat}')\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Output the test results\n",
        "print(\"\\n=== Final Test Results ===\")\n",
        "for feat in results:\n",
        "    print(f\"{feat.ljust(20)} Test Accuracy: {results[feat]['test_acc']:.4f}\")"
      ],
      "metadata": {
        "id": "KSnSuL0iXqDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#不知道在干什么\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "from torch_geometric.loader import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 加载数据集\n",
        "dataset = PygNodePropPredDataset(name=\"ogbn-arxiv\", root=\"dataset/\")\n",
        "data = dataset[0]\n",
        "split_idx = dataset.get_idx_split()\n",
        "\n",
        "# 数据集划分\n",
        "train_idx = split_idx[\"train\"]\n",
        "valid_idx = split_idx[\"valid\"]\n",
        "test_idx = split_idx[\"test\"]\n",
        "\n",
        "# 定义GraphSAGE模型\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# 初始化模型\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GraphSAGE(data.num_features, 256, dataset.num_classes).to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "# 优化器和损失函数\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 训练函数\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[train_idx], data.y[train_idx].squeeze())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# 验证函数\n",
        "def evaluate(mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct = pred[mask] == data.y[mask].squeeze()\n",
        "        acc = int(correct.sum()) / int(mask.sum())\n",
        "    return acc\n",
        "\n",
        "# 训练过程\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "test_accuracy = 0\n",
        "for epoch in range(100):  # 训练100轮\n",
        "    loss = train()\n",
        "    train_losses.append(loss)\n",
        "    val_acc = evaluate(valid_idx)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch}, Loss: {loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "test_accuracy = evaluate(test_idx)\n",
        "\n",
        "# 输出测试结果\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# 可视化训练损失和验证精度\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\", color=\"orange\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sTJXwRjIbImo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}